{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8dbeb3b-3fdb-4aff-bb6a-494a756bd183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          RentID  Year  RegionID     Region Province UnitID  \\\n",
      "0   Q05013T45654  2020       185  Montmagny   Quebec     IV   \n",
      "1   Q05020T15153  2020       186     Granby   Quebec      I   \n",
      "2   Q05020T18234  2020       185  Montmagny   Quebec      I   \n",
      "3   Q05021T21664  2020       185  Montmagny   Quebec     II   \n",
      "4   Q05021T28563  2020       186     Granby   Quebec     II   \n",
      "\n",
      "              UnitType StructureID  \\\n",
      "0  Three bedroom units           C   \n",
      "1       Bachelor units           D   \n",
      "2       Bachelor units           D   \n",
      "3    One bedroom units           D   \n",
      "4    One bedroom units           D   \n",
      "\n",
      "                                       StructureType  RentValue Status  \n",
      "0       Apartment structures of three units and over        685      T  \n",
      "1  Row and apartment structures of three units an...        455      T  \n",
      "2  Row and apartment structures of three units an...        513      T  \n",
      "3  Row and apartment structures of three units an...        496      T  \n",
      "4  Row and apartment structures of three units an...        586      T  \n",
      "(6617, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('FinalDataset-2.xlsx')\n",
    "\n",
    "#Print first few rows and shape of the dataset\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37c812b-a8ac-4862-9c09-eb059e2f7fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Root Mean Squared Error: 89.25821098094676\n",
      "     RegionID     Region Province UnitID             UnitType StructureID  \\\n",
      "0         185  Montmagny   Quebec     IV  Three bedroom units           C   \n",
      "1         186     Granby   Quebec      I       Bachelor units           D   \n",
      "2         185  Montmagny   Quebec      I       Bachelor units           D   \n",
      "3         185  Montmagny   Quebec     II    One bedroom units           D   \n",
      "4         186     Granby   Quebec     II    One bedroom units           D   \n",
      "..        ...        ...      ...    ...                  ...         ...   \n",
      "96        194   Val-d'Or   Quebec    III    Two bedroom units           C   \n",
      "97        194   Val-d'Or   Quebec     IV  Three bedroom units           C   \n",
      "98        194   Val-d'Or   Quebec      I       Bachelor units           D   \n",
      "99        194   Val-d'Or   Quebec     II    One bedroom units           D   \n",
      "100       194   Val-d'Or   Quebec    III    Two bedroom units           D   \n",
      "\n",
      "                                         StructureType Status  Year  \\\n",
      "0         Apartment structures of three units and over      T  2025   \n",
      "1    Row and apartment structures of three units an...      T  2025   \n",
      "2    Row and apartment structures of three units an...      T  2025   \n",
      "3    Row and apartment structures of three units an...      T  2025   \n",
      "4    Row and apartment structures of three units an...      T  2025   \n",
      "..                                                 ...    ...   ...   \n",
      "96        Apartment structures of three units and over      T  2025   \n",
      "97        Apartment structures of three units and over      T  2025   \n",
      "98   Row and apartment structures of three units an...      T  2025   \n",
      "99   Row and apartment structures of three units an...      T  2025   \n",
      "100  Row and apartment structures of three units an...      T  2025   \n",
      "\n",
      "     PredictedRentValue  \n",
      "0                884.98  \n",
      "1                554.91  \n",
      "2                497.76  \n",
      "3                555.40  \n",
      "4                683.52  \n",
      "..                  ...  \n",
      "96               876.93  \n",
      "97               900.19  \n",
      "98               649.27  \n",
      "99               634.61  \n",
      "100              876.43  \n",
      "\n",
      "[101 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define features and target variable\n",
    "#separate the features (X) from the target variable (y). \n",
    "#The RentID and RentValue columns are dropped from X, and RentValue is assigned to y as the target variable we want to predict.\n",
    "X = df.drop(columns=['RentID', 'RentValue'])\n",
    "y = df['RentValue']\n",
    "\n",
    "# Identify categorical and numerical columns - identify which columns in the dataset are categorical and which are numerical. \n",
    "categorical_columns = ['Region', 'Province', 'UnitType', 'StructureType', 'Status']\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline for categorical features \n",
    "# one-hot coding - converting categorical data into binary 1s or 0s\n",
    "#The parameter handle_unknown='ignore' ensures that any unknown categories in the test set are ignored.\n",
    "categorical_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features using one-hot encoding\n",
    "# 'StandardScaler' standardizes the features by removing the mean and scaling to unit variance\n",
    "numerical_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines- numerical features using standard scaling\n",
    "# combines the preprocessing steps for both categorical and numerical features into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_preprocessor, numerical_columns),\n",
    "        ('cat', categorical_preprocessor, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "#fits a RandomForestRegressor model. \n",
    "#The RandomForestRegressor is set with n_estimators=100 (the number of trees in the forest) and random_state=42 (for reproducibility\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Random Forest Regressor Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Generate new data for the years 2025, 2026, 2027 for all combinations of existing regions, unit types, etc.\n",
    "unique_combinations = df.drop(columns=['RentID', 'RentValue', 'Year']).drop_duplicates()\n",
    "\n",
    "years = [2025, 2026, 2027]\n",
    "new_data_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp_data = unique_combinations.copy()\n",
    "    temp_data['Year'] = year\n",
    "    new_data_list.append(temp_data)\n",
    "\n",
    "new_data = pd.concat(new_data_list, ignore_index=True)\n",
    "\n",
    "# Ensure the lengths of columns are correct by verifying each field\n",
    "assert all(len(new_data[col]) == len(new_data['Year']) for col in new_data.columns)\n",
    "\n",
    "# Since the new_data contains categorical values, they need to be preprocessed using the same encoder used in the training pipeline\n",
    "# Transform the new data using the preprocessor and make predictions\n",
    "new_data_preprocessed = model.named_steps['preprocessor'].transform(new_data)\n",
    "predictions = model.named_steps['regressor'].predict(new_data_preprocessed)\n",
    "\n",
    "# Add the predictions to the new_data DataFrame\n",
    "new_data['PredictedRentValue'] = predictions\n",
    "\n",
    "# Save the new dataset with predictions to a CSV file\n",
    "output_file_path = 'PredictedRentValues_2025_2027_regression_new.xlsx'\n",
    "new_data.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Return the new data as output\n",
    "print(new_data.head(101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f3f771-2d7d-4668-a6a7-dda4cba1ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 89.25821098094676\n",
      "     RegionID     Region Province UnitID             UnitType StructureID  \\\n",
      "0         185  Montmagny   Quebec     IV  Three bedroom units           C   \n",
      "1         186     Granby   Quebec      I       Bachelor units           D   \n",
      "2         185  Montmagny   Quebec      I       Bachelor units           D   \n",
      "3         185  Montmagny   Quebec     II    One bedroom units           D   \n",
      "4         186     Granby   Quebec     II    One bedroom units           D   \n",
      "..        ...        ...      ...    ...                  ...         ...   \n",
      "96        194   Val-d'Or   Quebec    III    Two bedroom units           C   \n",
      "97        194   Val-d'Or   Quebec     IV  Three bedroom units           C   \n",
      "98        194   Val-d'Or   Quebec      I       Bachelor units           D   \n",
      "99        194   Val-d'Or   Quebec     II    One bedroom units           D   \n",
      "100       194   Val-d'Or   Quebec    III    Two bedroom units           D   \n",
      "\n",
      "                                         StructureType Status  Year  \\\n",
      "0         Apartment structures of three units and over      T  2025   \n",
      "1    Row and apartment structures of three units an...      T  2025   \n",
      "2    Row and apartment structures of three units an...      T  2025   \n",
      "3    Row and apartment structures of three units an...      T  2025   \n",
      "4    Row and apartment structures of three units an...      T  2025   \n",
      "..                                                 ...    ...   ...   \n",
      "96        Apartment structures of three units and over      T  2025   \n",
      "97        Apartment structures of three units and over      T  2025   \n",
      "98   Row and apartment structures of three units an...      T  2025   \n",
      "99   Row and apartment structures of three units an...      T  2025   \n",
      "100  Row and apartment structures of three units an...      T  2025   \n",
      "\n",
      "     PredictedRentValue  \n",
      "0                884.98  \n",
      "1                554.91  \n",
      "2                497.76  \n",
      "3                555.40  \n",
      "4                683.52  \n",
      "..                  ...  \n",
      "96               876.93  \n",
      "97               900.19  \n",
      "98               649.27  \n",
      "99               634.61  \n",
      "100              876.43  \n",
      "\n",
      "[101 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define features and target variable\n",
    "#separate the features (X) from the target variable (y). \n",
    "#The RentID and RentValue columns are dropped from X, and RentValue is assigned to y as the target variable we want to predict.\n",
    "X = df.drop(columns=['RentID', 'RentValue'])\n",
    "y = df['RentValue']\n",
    "\n",
    "# Identify categorical and numerical columns - identify which columns in the dataset are categorical and which are numerical. \n",
    "categorical_columns = ['Region', 'Province', 'UnitType', 'StructureType', 'Status']\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline for categorical features \n",
    "# one-hot coding - converting categorical data into binary 1s or 0s\n",
    "#The parameter handle_unknown='ignore' ensures that any unknown categories in the test set are ignored.\n",
    "categorical_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features using one-hot encoding\n",
    "# 'StandardScaler' standardizes the features by removing the mean and scaling to unit variance\n",
    "numerical_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines- numerical features using standard scaling\n",
    "# combines the preprocessing steps for both categorical and numerical features into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_preprocessor, numerical_columns),\n",
    "        ('cat', categorical_preprocessor, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "#fits a RandomForestRegressor model. \n",
    "#The RandomForestRegressor is set with n_estimators=100 (the number of trees in the forest) and random_state=42 (for reproducibility\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Random Forest Regressor Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Generate new data for the years 2025, 2026, 2027 for all combinations of existing regions, unit types, etc.\n",
    "unique_combinations = df.drop(columns=['RentID', 'RentValue', 'Year']).drop_duplicates()\n",
    "\n",
    "years = [2025, 2026, 2027]\n",
    "new_data_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp_data = unique_combinations.copy()\n",
    "    temp_data['Year'] = year\n",
    "    new_data_list.append(temp_data)\n",
    "\n",
    "new_data = pd.concat(new_data_list, ignore_index=True)\n",
    "\n",
    "# Ensure the lengths of columns are correct by verifying each field\n",
    "assert all(len(new_data[col]) == len(new_data['Year']) for col in new_data.columns)\n",
    "\n",
    "# Since the new_data contains categorical values, they need to be preprocessed using the same encoder used in the training pipeline\n",
    "# Transform the new data using the preprocessor and make predictions\n",
    "new_data_preprocessed = model.named_steps['preprocessor'].transform(new_data)\n",
    "predictions = model.named_steps['regressor'].predict(new_data_preprocessed)\n",
    "\n",
    "# Add the predictions to the new_data DataFrame\n",
    "new_data['PredictedRentValue'] = predictions\n",
    "\n",
    "# Save the new dataset with predictions to a CSV file\n",
    "output_file_path = 'PredictedRentValues_2025_2027_regression.csv'\n",
    "new_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Return the new data as output\n",
    "print(new_data.head(101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f04869-dbe4-4260-bb45-60879ae90c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor Root Mean Squared Error: 153.99158515240853\n",
      "     RegionID     Region Province UnitID             UnitType StructureID  \\\n",
      "0         185  Montmagny   Quebec     IV  Three bedroom units           C   \n",
      "1         186     Granby   Quebec      I       Bachelor units           D   \n",
      "2         185  Montmagny   Quebec      I       Bachelor units           D   \n",
      "3         185  Montmagny   Quebec     II    One bedroom units           D   \n",
      "4         186     Granby   Quebec     II    One bedroom units           D   \n",
      "..        ...        ...      ...    ...                  ...         ...   \n",
      "96        194   Val-d'Or   Quebec    III    Two bedroom units           C   \n",
      "97        194   Val-d'Or   Quebec     IV  Three bedroom units           C   \n",
      "98        194   Val-d'Or   Quebec      I       Bachelor units           D   \n",
      "99        194   Val-d'Or   Quebec     II    One bedroom units           D   \n",
      "100       194   Val-d'Or   Quebec    III    Two bedroom units           D   \n",
      "\n",
      "                                         StructureType Status  Year  \\\n",
      "0         Apartment structures of three units and over      T  2025   \n",
      "1    Row and apartment structures of three units an...      T  2025   \n",
      "2    Row and apartment structures of three units an...      T  2025   \n",
      "3    Row and apartment structures of three units an...      T  2025   \n",
      "4    Row and apartment structures of three units an...      T  2025   \n",
      "..                                                 ...    ...   ...   \n",
      "96        Apartment structures of three units and over      T  2025   \n",
      "97        Apartment structures of three units and over      T  2025   \n",
      "98   Row and apartment structures of three units an...      T  2025   \n",
      "99   Row and apartment structures of three units an...      T  2025   \n",
      "100  Row and apartment structures of three units an...      T  2025   \n",
      "\n",
      "     PredictedRentValue  \n",
      "0            879.294153  \n",
      "1            549.503154  \n",
      "2            536.964979  \n",
      "3            674.112719  \n",
      "4            677.395318  \n",
      "..                  ...  \n",
      "96           822.012969  \n",
      "97           882.576752  \n",
      "98           549.503154  \n",
      "99           677.395318  \n",
      "100          822.012969  \n",
      "\n",
      "[101 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['RentID', 'RentValue'])\n",
    "y = df['RentValue']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['Region', 'Province', 'UnitType', 'StructureType', 'Status']\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "categorical_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features\n",
    "numerical_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_preprocessor, numerical_columns),\n",
    "        ('cat', categorical_preprocessor, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Gradient Boosting Regressor Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Generate new data for the years 2025, 2026, 2027\n",
    "unique_combinations = df.drop(columns=['RentID', 'RentValue', 'Year']).drop_duplicates()\n",
    "\n",
    "years = [2025, 2026, 2027]\n",
    "new_data_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp_data = unique_combinations.copy()\n",
    "    temp_data['Year'] = year\n",
    "    new_data_list.append(temp_data)\n",
    "\n",
    "new_data = pd.concat(new_data_list, ignore_index=True)\n",
    "\n",
    "# Ensure the lengths of columns are correct by verifying each field\n",
    "assert all(len(new_data[col]) == len(new_data['Year']) for col in new_data.columns)\n",
    "\n",
    "# Transform the new data using the preprocessor and make predictions\n",
    "new_data_preprocessed = model.named_steps['preprocessor'].transform(new_data)\n",
    "predictions = model.named_steps['regressor'].predict(new_data_preprocessed)\n",
    "\n",
    "# Add the predictions to the new_data DataFrame\n",
    "new_data['PredictedRentValue'] = predictions\n",
    "\n",
    "# Save the new dataset with predictions to a CSV file\n",
    "output_file_path = 'PredictedRentValues_2025_2027_gradient_boosting.csv'\n",
    "new_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Return the new data as output\n",
    "print(new_data.head(101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "804abe46-e61a-47b6-bf63-2957085985b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading xgboost-2.1.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.4.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.5-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from catboost) (3.8.0)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from catboost) (2.1.4)\n",
      "Requirement already satisfied: plotly in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\apeksha\\appdata\\roaming\\python\\python311\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\apeksha\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apeksha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->catboost) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\apeksha\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.2)\n",
      "Downloading xgboost-2.1.0-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/124.9 MB 2.0 MB/s eta 0:01:03\n",
      "   ---------------------------------------- 1.0/124.9 MB 12.1 MB/s eta 0:00:11\n",
      "    --------------------------------------- 2.4/124.9 MB 19.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 4.0/124.9 MB 23.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 4.5/124.9 MB 20.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 5.8/124.9 MB 21.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 7.2/124.9 MB 22.9 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 8.6/124.9 MB 24.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 10.1/124.9 MB 24.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 11.7/124.9 MB 28.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 12.1/124.9 MB 26.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 14.0/124.9 MB 27.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 15.5/124.9 MB 29.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 16.9/124.9 MB 29.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 19.2/124.9 MB 31.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 20.4/124.9 MB 31.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 22.6/124.9 MB 36.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 24.1/124.9 MB 34.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 26.1/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 27.9/124.9 MB 40.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 29.6/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 31.5/124.9 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 33.1/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 34.8/124.9 MB 38.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 36.3/124.9 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 37.7/124.9 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 39.6/124.9 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 41.9/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 43.6/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 45.4/124.9 MB 38.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 47.2/124.9 MB 40.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 48.9/124.9 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 50.7/124.9 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 52.3/124.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 54.0/124.9 MB 36.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 55.4/124.9 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 57.2/124.9 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 59.0/124.9 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.0/124.9 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 62.3/124.9 MB 34.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 63.4/124.9 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 64.2/124.9 MB 29.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 65.6/124.9 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 66.9/124.9 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 67.7/124.9 MB 24.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 69.5/124.9 MB 24.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 72.1/124.9 MB 23.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.2/124.9 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 75.9/124.9 MB 34.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 78.2/124.9 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 79.9/124.9 MB 43.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 81.2/124.9 MB 43.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 83.0/124.9 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 84.6/124.9 MB 38.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 86.5/124.9 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 88.2/124.9 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 90.0/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 91.3/124.9 MB 36.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 92.9/124.9 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 94.1/124.9 MB 34.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 95.5/124.9 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 97.4/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 99.0/124.9 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 102.4/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 103.4/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 104.4/124.9 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 105.7/124.9 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 108.1/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 109.8/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 111.2/124.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.4/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 113.1/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.8/124.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 117.2/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 118.6/124.9 MB 34.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 120.2/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  121.7/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.2/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 124.9/124.9 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading lightgbm-4.4.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.4/1.4 MB 46.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 30.5 MB/s eta 0:00:00\n",
      "Downloading catboost-1.2.5-cp311-cp311-win_amd64.whl (101.1 MB)\n",
      "   ---------------------------------------- 0.0/101.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.9/101.1 MB 39.7 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 3.4/101.1 MB 35.7 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 5.2/101.1 MB 37.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 6.9/101.1 MB 36.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 8.4/101.1 MB 38.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 10.2/101.1 MB 36.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 11.7/101.1 MB 36.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 13.7/101.1 MB 40.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 15.2/101.1 MB 36.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 17.3/101.1 MB 38.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 18.9/101.1 MB 38.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 21.1/101.1 MB 38.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 22.8/101.1 MB 40.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 24.6/101.1 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 26.4/101.1 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 26.6/101.1 MB 38.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 26.6/101.1 MB 38.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 26.6/101.1 MB 38.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 29.9/101.1 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 33.2/101.1 MB 32.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 35.8/101.1 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 37.6/101.1 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 39.5/101.1 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 41.4/101.1 MB 50.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 41.5/101.1 MB 46.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 42.6/101.1 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 44.3/101.1 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 46.0/101.1 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 48.1/101.1 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 49.5/101.1 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 51.7/101.1 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 53.2/101.1 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 54.6/101.1 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 56.2/101.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 57.8/101.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 59.3/101.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 60.9/101.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 63.0/101.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 64.7/101.1 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 66.5/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 67.8/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 69.5/101.1 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 71.6/101.1 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 73.5/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 75.1/101.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 76.7/101.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 78.2/101.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 80.0/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 81.8/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 83.7/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 85.3/101.1 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 87.2/101.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 88.9/101.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 90.7/101.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 92.3/101.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.2/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 95.8/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.7/101.1 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/101.1 MB 36.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  100.8/101.1 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.1/101.1 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.1/101.1 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.1/47.1 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: graphviz, xgboost, lightgbm, catboost\n",
      "Successfully installed catboost-1.2.5 graphviz-0.20.3 lightgbm-4.4.0 xgboost-2.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fd888dd-7c2b-4423-80ae-7e8088ecc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Regressor RMSE: 99.15600569179853\n",
      "   RegionID     Region Province UnitID             UnitType StructureID  \\\n",
      "0       185  Montmagny   Quebec     IV  Three bedroom units           C   \n",
      "1       186     Granby   Quebec      I       Bachelor units           D   \n",
      "2       185  Montmagny   Quebec      I       Bachelor units           D   \n",
      "3       185  Montmagny   Quebec     II    One bedroom units           D   \n",
      "4       186     Granby   Quebec     II    One bedroom units           D   \n",
      "\n",
      "                                       StructureType Status  Year  \\\n",
      "0       Apartment structures of three units and over      T  2025   \n",
      "1  Row and apartment structures of three units an...      T  2025   \n",
      "2  Row and apartment structures of three units an...      T  2025   \n",
      "3  Row and apartment structures of three units an...      T  2025   \n",
      "4  Row and apartment structures of three units an...      T  2025   \n",
      "\n",
      "   PredictedRentValue  \n",
      "0          875.939861  \n",
      "1          574.984552  \n",
      "2          516.835092  \n",
      "3          636.797626  \n",
      "4          715.061370  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['RentID', 'RentValue'])\n",
    "y = df['RentValue']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['Region', 'Province', 'UnitType', 'StructureType', 'Status']\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "categorical_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features\n",
    "numerical_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_preprocessor, numerical_columns),\n",
    "        ('cat', categorical_preprocessor, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', CatBoostRegressor(iterations=200, random_state=42, verbose=0))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"CatBoost Regressor RMSE: {rmse}\")\n",
    "\n",
    "# Generate new data for the years 2025, 2026, 2027 for all combinations of existing regions, unit types, etc.\n",
    "unique_combinations = df.drop(columns=['RentID', 'RentValue', 'Year']).drop_duplicates()\n",
    "\n",
    "years = [2025, 2026, 2027]\n",
    "new_data_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp_data = unique_combinations.copy()\n",
    "    temp_data['Year'] = year\n",
    "    new_data_list.append(temp_data)\n",
    "\n",
    "new_data = pd.concat(new_data_list, ignore_index=True)\n",
    "\n",
    "# Ensure the lengths of columns are correct by verifying each field\n",
    "assert all(len(new_data[col]) == len(new_data['Year']) for col in new_data.columns)\n",
    "\n",
    "# Since the new_data contains categorical values, they need to be preprocessed using the same encoder used in the training pipeline\n",
    "# Transform the new data using the preprocessor and make predictions\n",
    "new_data_preprocessed = model.named_steps['preprocessor'].transform(new_data)\n",
    "predictions = model.named_steps['regressor'].predict(new_data_preprocessed)\n",
    "\n",
    "# Add the predictions to the new_data DataFrame\n",
    "new_data['PredictedRentValue'] = predictions\n",
    "\n",
    "# Save the new dataset with predictions to a CSV file\n",
    "output_file_path = 'PredictedRentValues_2025_2027_catboost.csv'\n",
    "new_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Return the new data as output\n",
    "print(new_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1cf1093-eb68-47ef-9fc0-35d7b5fc166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 446\n",
      "[LightGBM] [Info] Number of data points in the train set: 3970, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score 1018.056927\n",
      "LightGBM Regressor RMSE: 101.72071691487274\n",
      "   RegionID     Region Province UnitID             UnitType StructureID  \\\n",
      "0       185  Montmagny   Quebec     IV  Three bedroom units           C   \n",
      "1       186     Granby   Quebec      I       Bachelor units           D   \n",
      "2       185  Montmagny   Quebec      I       Bachelor units           D   \n",
      "3       185  Montmagny   Quebec     II    One bedroom units           D   \n",
      "4       186     Granby   Quebec     II    One bedroom units           D   \n",
      "\n",
      "                                       StructureType Status  Year  \\\n",
      "0       Apartment structures of three units and over      T  2025   \n",
      "1  Row and apartment structures of three units an...      T  2025   \n",
      "2  Row and apartment structures of three units an...      T  2025   \n",
      "3  Row and apartment structures of three units an...      T  2025   \n",
      "4  Row and apartment structures of three units an...      T  2025   \n",
      "\n",
      "   PredictedRentValue  \n",
      "0          858.955133  \n",
      "1          590.682851  \n",
      "2          479.690444  \n",
      "3          569.870653  \n",
      "4          726.042783  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['RentID', 'RentValue'])\n",
    "y = df['RentValue']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['Region', 'Province', 'UnitType', 'StructureType', 'Status']\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "categorical_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features\n",
    "numerical_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_preprocessor, numerical_columns),\n",
    "        ('cat', categorical_preprocessor, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LGBMRegressor(n_estimators=200, random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets (60-40 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"LightGBM Regressor RMSE: {rmse}\")\n",
    "\n",
    "# Generate new data for the years 2025, 2026, 2027 for all combinations of existing regions, unit types, etc.\n",
    "unique_combinations = df.drop(columns=['RentID', 'RentValue', 'Year']).drop_duplicates()\n",
    "\n",
    "years = [2025, 2026, 2027]\n",
    "new_data_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp_data = unique_combinations.copy()\n",
    "    temp_data['Year'] = year\n",
    "    new_data_list.append(temp_data)\n",
    "\n",
    "new_data = pd.concat(new_data_list, ignore_index=True)\n",
    "\n",
    "# Ensure the lengths of columns are correct by verifying each field\n",
    "assert all(len(new_data[col]) == len(new_data['Year']) for col in new_data.columns)\n",
    "\n",
    "# Since the new_data contains categorical values, they need to be preprocessed using the same encoder used in the training pipeline\n",
    "# Transform the new data using the preprocessor and make predictions\n",
    "new_data_preprocessed = model.named_steps['preprocessor'].transform(new_data)\n",
    "predictions = model.named_steps['regressor'].predict(new_data_preprocessed)\n",
    "\n",
    "# Add the predictions to the new_data DataFrame\n",
    "new_data['PredictedRentValue'] = predictions\n",
    "\n",
    "# Save the new dataset with predictions to a CSV file\n",
    "output_file_path = 'PredictedRentValues_2025_2027_lgbm.csv'\n",
    "new_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Return the new data as output\n",
    "print(new_data.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
